{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SII_Task_0.71f1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Bidirectional,GRU,LSTM,Dense,Embedding,RepeatVector,TimeDistributed,Activation,Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tensorflow.keras import activations\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from string import punctuation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Markdown, display\n",
        "import random\n",
        "from random import shuffle\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from random import Random\n",
        "from numpy.random import seed\n",
        "import time\n"
      ],
      "metadata": {
        "id": "RyBwnobbWu9l"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "csIYMpDHQGvS"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_json('train.json')\n",
        "\n",
        "final_test = pd.read_json('test_wor.json')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBw8p7CXSEZP",
        "outputId": "c9403512-b692-42d4-dfe3-b06f89916f64"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"racai/distilbert-base-romanian-uncased\")\n",
        "model = AutoModel.from_pretrained(\"racai/distilbert-base-romanian-uncased\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xxLmQAfSDMj",
        "outputId": "18f556c9-41d5-4f25-bdf9-15d05a7d8778"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at racai/distilbert-base-romanian-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def tokenize(sentences, tokenizer):\n",
        "    input_ids= []\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer.encode(sentence, add_special_tokens=True, max_length=512, truncation=True, padding=\"max_length\")\n",
        "        input_ids.append(inputs)       \n",
        "        \n",
        "    return np.asarray(input_ids, dtype='int32')"
      ],
      "metadata": {
        "id": "YHZ7bBw-TMZM"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX = df[\"text\"]\n",
        "for idx, x in enumerate(trainX):\n",
        "  trainX[idx] = x.lower()\n",
        "r1 = tokenize(trainX, tokenizer)\n",
        "\n",
        "final_testX = final_test[\"text\"]\n",
        "for idx, x in enumerate(final_testX):\n",
        "  final_testX[idx] = x.lower()\n",
        "final_r1 = tokenize(final_testX, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNJaekQNT5to",
        "outputId": "441ae27b-58e3-4a20-a5d2-c0372f64ef1b"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = r1\n",
        "Y = df[\"rating\"]\n",
        "\n",
        "dataX = []\n",
        "dataY = []\n",
        "count5 = 0\n",
        "for x, y in zip(X, Y):\n",
        "  if y == 5:\n",
        "    count5 += 1\n",
        "\n",
        "  # if y == 5 and count5 <= 3200:\n",
        "  #   dataX.append(x)\n",
        "  #   dataY.append(y)\n",
        "  \n",
        "  # if y!= 5:\n",
        "  #   dataX.append(x)\n",
        "  #   dataY.append(y)\n",
        "  \n",
        "  dataX.append(x)\n",
        "  dataY.append(y)\n",
        "  \n",
        "\n",
        "test = dict(Counter(dataY))\n",
        "test[0] = 0\n",
        "\n",
        "\n",
        "\n",
        "final_X = final_r1"
      ],
      "metadata": {
        "id": "Tmzf9pc9U2Qr"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataX = np.array(dataX)\n",
        "dataY = np.array(dataY)\n",
        "final_X = np.array(final_X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "new_X_train = []\n",
        "new_Y_train = []\n",
        "count5=0\n",
        "for x, y in zip(X_train, y_train):\n",
        "  if y == [5]:\n",
        "    count5 += 1\n",
        "\n",
        "  # if y == 5 and count5 <= 4000:\n",
        "  #   new_X_train.append(x)\n",
        "  #   new_Y_train.append(y)\n",
        "  \n",
        "  # if y!= 5:\n",
        "  #   new_X_train.append(x)\n",
        "  #   new_Y_train.append(y)\n",
        "  new_X_train.append(x)\n",
        "  new_Y_train.append(y)\n",
        "\n",
        "\n",
        "X_train = np.array(new_X_train)\n",
        "y_train = np.array(new_Y_train)\n",
        "\n",
        "\n",
        "print(dict(Counter(y_train)))\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n"
      ],
      "metadata": {
        "id": "LK493a3YU9rI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a2575e-0b92-4e40-f489-7f8832f9c89e"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{5: 11081, 1: 1220, 4: 2173, 3: 1153, 2: 483}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_output(sequences, vocab_size):\n",
        "    # one hot encode target sequence\n",
        "    ylist = list()\n",
        "    for sequence in sequences:\n",
        "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = np.array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y"
      ],
      "metadata": {
        "id": "FeV8UcCkfmrM"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n"
      ],
      "metadata": {
        "id": "4ddO2NXnbrnZ"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_f1(y_true, y_pred): \n",
        "  return metrics.f1_score(y_true, y_pred, average='weighted')"
      ],
      "metadata": {
        "id": "egvU38whjPD0"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = np.array(X_train)"
      ],
      "metadata": {
        "id": "uKjnXweCYO6z"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = np.array(X_train.tolist())"
      ],
      "metadata": {
        "id": "IpB4ckpWY_fu"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize training history\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "def plot_history(history):\n",
        "  # print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "J6NMcZbxbPRF"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{5: 11081, 1: 1220, 4: 2173, 3: 1153, 2: 483}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGukbPHnhfGO",
        "outputId": "3355e5cf-ff95-4636-e7e8-b82e220811d1"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 1220, 2: 483, 3: 1153, 4: 2173, 5: 11081}"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_new ={\n",
        "    0:1,\n",
        "    1:5,\n",
        "    2:10,\n",
        "    3:5,\n",
        "    4:3,\n",
        "    5:1\n",
        "}"
      ],
      "metadata": {
        "id": "dJtVPhd9ZuOd"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import json\n",
        "global max_f1\n",
        "max_f1 = 0\n",
        "def predict_store_data(model, modelname):\n",
        "  predictions = model.predict(X_test)\n",
        "\n",
        "  y_pred = []\n",
        "  for x in predictions:\n",
        "    y_pred.append(np.argmax(x))\n",
        "\n",
        "  y_test2 = []\n",
        "  for x in y_test:\n",
        "    y_test2.append(np.argmax(x))\n",
        "\n",
        "  f1_weighted = get_f1(y_test2, y_pred)\n",
        "\n",
        "\n",
        "  final_predictions = model.predict(final_X)\n",
        "  y_final = []\n",
        "  for x in final_predictions:\n",
        "    y_final.append(np.argmax(x))\n",
        "\n",
        "  for idx, y in enumerate(y_final):\n",
        "    final_test[\"rating\"][idx] = y\n",
        "\n",
        "  with open('/content/drive/MyDrive/SII/results.txt', 'a') as f:\n",
        "          f.write(\"%s; %s;\\n\" % (modelname, f1_weighted))\n",
        "\n",
        "  global max_f1\n",
        "  if(f1_weighted > max_f1):\n",
        "    max_f1 = f1_weighted\n",
        "    final_test.to_json(path_or_buf=\"/content/drive/MyDrive/SII/\"+ modelname+\".json\" , orient=\"records\")\n",
        "  \n",
        "  return f1_weighted\n"
      ],
      "metadata": {
        "id": "3w8lY_QZkNGB"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Neural Network\n",
        "def createModel(params):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(Embedding(50000, params[\"emb\"], input_length=512, mask_zero=True))\n",
        "  model.add(Bidirectional(LSTM(params[\"lstm\"]), merge_mode='concat'))\n",
        "  model.add(Dropout(params[\"drop\"]))\n",
        "  model.add(tf.keras.layers.Dense(params[\"dense\"]))\n",
        "  model.add(Dropout(params[\"drop\"]))\n",
        "  model.add(Dense(6, activation=\"softmax\"))\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=params[\"lr\"])\n",
        "  model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# model = createModel()\n",
        "# # print(model.summary())\n",
        "# history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_split=0.1,\n",
        "#         verbose=1,\n",
        "#         # class_weight = test,\n",
        "#         class_weight = test_new,\n",
        "#         callbacks=[\n",
        "#                       EarlyStopping(\n",
        "#                       monitor='val_accuracy',\n",
        "#                       patience=5,\n",
        "#                       restore_best_weights=True\n",
        "#                   )\n",
        "#           ])\n",
        "\n",
        "# plot_history(history)\n",
        "\n",
        "lrs = [2e-4]\n",
        "embs = [128]\n",
        "drops = [0.8]\n",
        "lstms = [10]\n",
        "denses = [16]\n",
        "model = None\n",
        "for lr in lrs:\n",
        "  for emb in embs:\n",
        "    for drop in drops:\n",
        "      for lstm in lstms:\n",
        "        for dense in denses:\n",
        "          seed(42)\n",
        "          tf.random.set_seed(42)\n",
        "          random.seed(42)\n",
        "\n",
        "          params = {\n",
        "              \"lr\":lr,\n",
        "              \"emb\":emb,\n",
        "              \"drop\":drop,\n",
        "              \"lstm\":lstm,\n",
        "              \"dense\":dense\n",
        "          }\n",
        "\n",
        "          model = createModel(params)\n",
        "\n",
        "          history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_split=0.1,\n",
        "          verbose=1,\n",
        "          class_weight = test_new,\n",
        "          callbacks=[\n",
        "                        EarlyStopping(\n",
        "                        monitor='val_loss',\n",
        "                        patience=4,\n",
        "                        restore_best_weights=True\n",
        "                    )\n",
        "            ])\n",
        "          \n",
        "          f1_score = predict_store_data(model, \"BILSTM2_test;\" + json.dumps(params))\n",
        "          print(f1_score, json.dumps(params))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8EUSc9hXLRK",
        "outputId": "2fa69f87-ca85-458f-8f68-d6eceacc333c"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "454/454 [==============================] - 24s 37ms/step - loss: 3.7122 - accuracy: 0.4783 - val_loss: 1.5422 - val_accuracy: 0.6853\n",
            "Epoch 2/100\n",
            "454/454 [==============================] - 15s 32ms/step - loss: 3.5872 - accuracy: 0.5765 - val_loss: 1.4157 - val_accuracy: 0.6853\n",
            "Epoch 3/100\n",
            "454/454 [==============================] - 15s 32ms/step - loss: 3.4552 - accuracy: 0.5916 - val_loss: 1.2724 - val_accuracy: 0.6983\n",
            "Epoch 4/100\n",
            "454/454 [==============================] - 15s 32ms/step - loss: 3.3443 - accuracy: 0.6050 - val_loss: 1.1488 - val_accuracy: 0.7039\n",
            "Epoch 5/100\n",
            "454/454 [==============================] - 16s 35ms/step - loss: 3.2276 - accuracy: 0.6201 - val_loss: 1.0635 - val_accuracy: 0.7027\n",
            "Epoch 6/100\n",
            "454/454 [==============================] - 15s 33ms/step - loss: 3.1388 - accuracy: 0.6402 - val_loss: 1.1390 - val_accuracy: 0.6685\n",
            "Epoch 7/100\n",
            "454/454 [==============================] - 15s 33ms/step - loss: 3.0543 - accuracy: 0.6538 - val_loss: 0.9276 - val_accuracy: 0.6989\n",
            "Epoch 8/100\n",
            "454/454 [==============================] - 15s 34ms/step - loss: 2.9731 - accuracy: 0.6596 - val_loss: 0.9191 - val_accuracy: 0.7052\n",
            "Epoch 9/100\n",
            "454/454 [==============================] - 15s 32ms/step - loss: 2.9071 - accuracy: 0.6716 - val_loss: 0.9317 - val_accuracy: 0.6952\n",
            "Epoch 10/100\n",
            "454/454 [==============================] - 16s 35ms/step - loss: 2.8162 - accuracy: 0.6754 - val_loss: 0.8575 - val_accuracy: 0.6989\n",
            "Epoch 11/100\n",
            "454/454 [==============================] - 15s 32ms/step - loss: 2.7234 - accuracy: 0.6795 - val_loss: 0.8342 - val_accuracy: 0.7138\n",
            "Epoch 12/100\n",
            "454/454 [==============================] - 15s 32ms/step - loss: 2.6713 - accuracy: 0.6628 - val_loss: 0.9162 - val_accuracy: 0.6747\n",
            "Epoch 13/100\n",
            "454/454 [==============================] - 15s 32ms/step - loss: 2.5981 - accuracy: 0.6763 - val_loss: 0.8708 - val_accuracy: 0.6859\n",
            "Epoch 14/100\n",
            "454/454 [==============================] - 15s 32ms/step - loss: 2.5434 - accuracy: 0.6696 - val_loss: 0.8689 - val_accuracy: 0.6921\n",
            "Epoch 15/100\n",
            "454/454 [==============================] - 15s 32ms/step - loss: 2.4950 - accuracy: 0.6718 - val_loss: 0.8887 - val_accuracy: 0.7076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6475676061480602 {\"lr\": 0.0002, \"emb\": 128, \"drop\": 0.8, \"lstm\": 10, \"dense\": 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = tokenizer.encode(\"Microfon de calitate superioara. Izoralea fonica a castilor este foarte buna.\", add_special_tokens=True, max_length=512, truncation=True, padding=\"max_length\")\n",
        "predictions = model.predict([test_input])\n",
        "print(np.argmax(predictions[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRbMUBpuXl2g",
        "outputId": "1f5c4317-f130-480e-bb10-51cf29bf30b5"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    }
  ]
}